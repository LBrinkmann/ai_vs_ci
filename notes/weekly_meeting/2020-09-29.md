# Results

## Feedback

![](../../runs/feedback/test1/plot/rewards.png)

### avg_catch_cor, avg_catch
* adding the average coordination as input in a later layer seems to help the CI to keep being focused on coordination (avg_catch_cor)
* if only catch is added the CI focuses more on not being catched (avg_catch)
* the reward of the CI is largely uneffected by neither

### neighbor_catch, ind_catch
* adding a booling for each neighbor (and self) if being catched at the beginning of the network does help to avoid being catched (neighbor_catch)
* with this information agents manage to increasingly outperform the AI at low learning rates
* similar observations (with lower magnitude) can be made when the self catch is being added at a later layer in the network (ind_catch)

## Asymmetric Learning Rate

![](../../runs/attention/asymmetric_lr2/plot/reward.png)


* when AI learns faster then CI: CI wins
* when both learn with same learning rate: CI wins mostly, but for lr=0.0003
* when CI learns to fast: AI wins
* **we do not really see that CI benefit from faster learning**

## Asymmetric Learning Rate and hidden size

### AI
* lr: 0.001
* hidden_size: 100

![](../../runs/attention/asymmetric_hidden_lr/plot/reward.png)


## Asymmetric Reward

![](../../runs/attention/asymmetric_reward/plot/reward_metrics.png)

![](../../runs/attention/asymmetric_reward/plot/ci_reward.png)


# Ongoing

* Seeds for mixed strategy
* Cheap Talk

# Next 

* shared weights